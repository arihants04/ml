To run this code pip install numpy, pandas, scikit-learn, matplotlib, ucimlrepo
After seeing my results I can conclude that the relu activation function with a learning rate of 0.1, max iterations of 10000, and 3 hidden layers provided the best overall performance.
This was something that I expected as Relu is proven to be the best activation function however I did not expect it to perform best with a learning rate of 0.1.
